{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1: What is a Decision Tree, and how does it work in the context of\n",
        "classification?**\n",
        "\n",
        "- A Decision Tree is a supervised learning algorithm used for classification and regression tasks. It works like a flowchart, where each internal node represents a decision based on a feature, each branch represents an outcome of that decision, and each leaf node represents a final class label or output.\n",
        "\n",
        " **How It Works (for Classification):**\n",
        " 1. Start with the entire dataset.\n",
        "    - The algorithm begins at the root node and considers all features.\n",
        "\n",
        "2. Select the best feature to split the data.\n",
        "    - It chooses the feature that best separates the classes using a splitting criterion, such as:\n",
        "\n",
        "      - Gini Impurity\n",
        "\n",
        "      - Entropy / Information Gain\n",
        "\n",
        "3. Split the dataset into subsets.\n",
        "    - The data is divided based on the values of the selected feature.\n",
        "\n",
        "4. Repeat the process recursively.\n",
        "    - For each subset, the algorithm again selects the best feature and splits further — forming a tree-like structure.\n",
        "\n",
        "5. Stop when a stopping condition is met.\n",
        "\n",
        "    - All data points in a node belong to the same class, or\n",
        "\n",
        "    - No remaining features to split, or\n",
        "\n",
        "    - The maximum tree depth is reached.\n",
        "\n",
        "6. Assign class labels to leaf nodes.\n",
        "    - Each leaf node gives the final classification output.\n",
        "\n"
      ],
      "metadata": {
        "id": "hddITCGZq1OJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 2: Explain the concepts of Gini Impurity and Entropy as impurity measures.\n",
        "How do they impact the splits in a Decision Tree?**  \n",
        "\n",
        "- In a Decision Tree, impurity measures are used to decide which feature to split on at each step. The goal is to choose the feature that creates the purest child nodes — meaning the data in each branch is as homogeneous (single class) as possible.\n",
        "\n",
        " - Two commonly used impurity measures are Gini Impurity and Entropy.\n",
        "\n",
        "1. Gini Impurity\n",
        "\n",
        " - Definition:\n",
        "Gini Impurity measures how often a randomly chosen sample from the dataset would be incorrectly classified if it were labeled according to the distribution of labels in that subset.\n",
        "\n",
        " - Formula:\n",
        "\n",
        "𝐺\n",
        "𝑖\n",
        "𝑛\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "−\n",
        "∑\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "𝑐\n",
        "𝑝\n",
        "𝑖\n",
        "2\n",
        "Gini=1−\n",
        "i=1\n",
        "∑\n",
        "c\n",
        "\t​\n",
        "\n",
        "p\n",
        "i\n",
        "2\n",
        "\t​\n",
        "\n",
        "\n",
        "where\n",
        "\n",
        "𝑐\n",
        "c = number of classes\n",
        "\n",
        "𝑝\n",
        "𝑖\n",
        "p\n",
        "i\n",
        "\t​\n",
        "\n",
        " = probability (proportion) of class i in the node\n",
        "\n",
        "Interpretation:\n",
        "\n",
        "Gini = 0 → perfectly pure node (only one class)\n",
        "\n",
        "Gini = 0.5 → maximum impurity (two classes equally mixed)\n",
        "\n",
        "Example:\n",
        "If a node has 4 samples: 3 “Yes” and 1 “No”\n",
        "\n",
        "𝑝\n",
        "(\n",
        "𝑌\n",
        "𝑒\n",
        "𝑠\n",
        ")\n",
        "=\n",
        "3\n",
        "/\n",
        "4\n",
        "=\n",
        "0.75\n",
        ",\n",
        "𝑝\n",
        "(\n",
        "𝑁\n",
        "𝑜\n",
        ")\n",
        "=\n",
        "0.25\n",
        "p(Yes)=3/4=0.75,p(No)=0.25\n",
        "𝐺\n",
        "𝑖\n",
        "𝑛\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "−\n",
        "(\n",
        "0.75\n",
        "2\n",
        "+\n",
        "0.25\n",
        "2\n",
        ")\n",
        "=\n",
        "1\n",
        "−\n",
        "(\n",
        "0.5625\n",
        "+\n",
        "0.0625\n",
        ")\n",
        "=\n",
        "0.375\n",
        "Gini=1−(0.75\n",
        "2\n",
        "+0.25\n",
        "2\n",
        ")=1−(0.5625+0.0625)=0.375\n",
        "\n",
        "→ Lower Gini = better split (purer node)\n",
        "\n",
        "\n",
        "\n",
        "2. Entropy (Information Gain)\n",
        "\n",
        "Definition:\n",
        "Entropy measures the amount of disorder or uncertainty in the dataset.\n",
        "\n",
        "Formula:\n",
        "\n",
        "𝐸\n",
        "𝑛\n",
        "𝑡\n",
        "𝑟\n",
        "𝑜\n",
        "𝑝\n",
        "𝑦\n",
        "=\n",
        "−\n",
        "∑\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "𝑐\n",
        "𝑝\n",
        "𝑖\n",
        "log\n",
        "⁡\n",
        "2\n",
        "(\n",
        "𝑝\n",
        "𝑖\n",
        ")\n",
        "Entropy=−\n",
        "i=1\n",
        "∑\n",
        "c\n",
        "\t​\n",
        "\n",
        "p\n",
        "i\n",
        "\t​\n",
        "\n",
        "log\n",
        "2\n",
        "\t​\n",
        "\n",
        "(p\n",
        "i\n",
        "\t​\n",
        "\n",
        ")\n",
        "\n",
        "where\n",
        "\n",
        "𝑝\n",
        "𝑖\n",
        "p\n",
        "i\n",
        "\t​\n",
        "\n",
        " = probability of class i\n",
        "\n",
        "Interpretation:\n",
        "\n",
        "Entropy = 0 → pure node\n",
        "\n",
        "Entropy = 1 → maximum impurity (50-50 split for binary classes)\n",
        "\n",
        "Example:\n",
        "Using the same data (3 “Yes”, 1 “No”):\n",
        "\n",
        "𝐸\n",
        "𝑛\n",
        "𝑡\n",
        "𝑟\n",
        "𝑜\n",
        "𝑝\n",
        "𝑦\n",
        "=\n",
        "−\n",
        "(\n",
        "0.75\n",
        "log\n",
        "⁡\n",
        "2\n",
        "0.75\n",
        "+\n",
        "0.25\n",
        "log\n",
        "⁡\n",
        "2\n",
        "0.25\n",
        ")\n",
        "Entropy=−(0.75log\n",
        "2\n",
        "\t​\n",
        "\n",
        "0.75+0.25log\n",
        "2\n",
        "\t​\n",
        "\n",
        "0.25)\n",
        "=\n",
        "−\n",
        "(\n",
        "0.75\n",
        "×\n",
        "−\n",
        "0.415\n",
        "+\n",
        "0.25\n",
        "×\n",
        "−\n",
        "2\n",
        ")\n",
        "=−(0.75×−0.415+0.25×−2)\n",
        "=\n",
        "0.811\n",
        "=0.811\n",
        "\n",
        "\n",
        "\n",
        "How They Impact Splits\n",
        "\n",
        "At each node, the Decision Tree tries different splits using each feature.\n",
        "\n",
        "For each split, it calculates the weighted average impurity of the resulting child nodes.\n",
        "\n",
        "The feature producing the lowest impurity (or highest information gain) is chosen for splitting.\n",
        "\n",
        "Information Gain (based on Entropy):\n",
        "\n",
        "𝐼\n",
        "𝑛\n",
        "𝑓\n",
        "𝑜\n",
        "𝑟\n",
        "𝑚\n",
        "𝑎\n",
        "𝑡\n",
        "𝑖\n",
        "𝑜\n",
        "𝑛\n",
        "\n",
        "𝐺\n",
        "𝑎\n",
        "𝑖\n",
        "𝑛\n",
        "=\n",
        "𝐸\n",
        "𝑛\n",
        "𝑡\n",
        "𝑟\n",
        "𝑜\n",
        "𝑝\n",
        "𝑦\n",
        "𝑝\n",
        "𝑎\n",
        "𝑟\n",
        "𝑒\n",
        "𝑛\n",
        "𝑡\n",
        "−\n",
        "∑\n",
        "𝑛\n",
        "𝑐\n",
        "ℎ\n",
        "𝑖\n",
        "𝑙\n",
        "𝑑\n",
        "𝑛\n",
        "𝑝\n",
        "𝑎\n",
        "𝑟\n",
        "𝑒\n",
        "𝑛\n",
        "𝑡\n",
        "×\n",
        "𝐸\n",
        "𝑛\n",
        "𝑡\n",
        "𝑟\n",
        "𝑜\n",
        "𝑝\n",
        "𝑦\n",
        "𝑐\n",
        "ℎ\n",
        "𝑖\n",
        "𝑙\n",
        "𝑑\n",
        "Information Gain=Entropy\n",
        "parent\n",
        "\t​\n",
        "\n",
        "−∑\n",
        "n\n",
        "parent\n",
        "\t​\n",
        "\n",
        "n\n",
        "child\n",
        "\t​\n",
        "\n",
        "\t​\n",
        "\n",
        "×Entropy\n",
        "child\n",
        "\t​\n",
        "\n",
        "\n",
        "The higher the information gain → the better the split."
      ],
      "metadata": {
        "id": "B0xF9RyisHmc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 3: What is the difference between Pre-Pruning and Post-Pruning in Decision\n",
        "Trees? Give one practical advantage of using each**\n",
        "\n",
        "- In Decision Trees, pruning is a technique used to prevent overfitting — when a model becomes too complex and starts memorizing training data instead of generalizing patterns.\n",
        "\n",
        "There are two types of pruning methods: Pre-Pruning and Post-Pruning.\n",
        "1. Pre-Pruning (Early Stopping)\n",
        "\n",
        "Definition:\n",
        "Pre-pruning means stopping the tree growth early — before it becomes too deep or complex.\n",
        "\n",
        "How it works:\n",
        "The algorithm stops splitting a node if:\n",
        "\n",
        "The information gain (or impurity reduction) is below a threshold, or\n",
        "\n",
        "The node size is too small, or\n",
        "\n",
        "The tree depth reaches a maximum limit, etc.\n",
        "\n",
        "Example Parameters in scikit-learn:\n",
        "\n",
        "max_depth\n",
        "\n",
        "min_samples_split\n",
        "\n",
        "min_samples_leaf\n",
        "\n",
        "max_leaf_nodes\n",
        "\n",
        "Practical Advantage:\n",
        " Faster training and simpler model.\n",
        "Since the tree stops early, it’s less complex and faster to build — suitable for large datasets or real-time predictions.\n",
        "\n",
        "2. Post-Pruning (Cost Complexity Pruning)\n",
        "\n",
        "Definition:\n",
        "Post-pruning means first growing the full tree, and then removing the less important branches that do not contribute much to accuracy.\n",
        "\n",
        "How it works:\n",
        "\n",
        "Build the complete tree (possibly overfitted).\n",
        "\n",
        "Evaluate subtrees using a validation set or cost-complexity measure (α).\n",
        "\n",
        "Prune branches that reduce accuracy only slightly or increase error.\n",
        "\n",
        "Example in scikit-learn:\n",
        "\n",
        "Controlled by ccp_alpha (cost complexity pruning parameter)."
      ],
      "metadata": {
        "id": "bcm7epMAs6LJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 4: What is Information Gain in Decision Trees, and why is it important for\n",
        "choosing the best split?**\n",
        "\n",
        "- Information Gain (IG) is a key concept in Decision Trees used to decide which feature to split on at each node.\n",
        "It measures how much “information” or “purity” is gained after a dataset is split based on a particular feature.\n",
        "\n",
        "In simple terms:\n",
        "👉 Information Gain tells us how well a feature separates the classes.\n",
        "The higher the Information Gain, the better that feature is for splitting.\n",
        "\n",
        "📘 Definition:\n",
        "\n",
        "Information Gain is the reduction in entropy (disorder) achieved by splitting the data based on a given feature.\n",
        "\n",
        "𝐼\n",
        "𝑛\n",
        "𝑓\n",
        "𝑜\n",
        "𝑟\n",
        "𝑚\n",
        "𝑎\n",
        "𝑡\n",
        "𝑖\n",
        "𝑜\n",
        "𝑛\n",
        "\n",
        "𝐺\n",
        "𝑎\n",
        "𝑖\n",
        "𝑛\n",
        "=\n",
        "𝐸\n",
        "𝑛\n",
        "𝑡\n",
        "𝑟\n",
        "𝑜\n",
        "𝑝\n",
        "𝑦\n",
        "(\n",
        "𝑃\n",
        "𝑎\n",
        "𝑟\n",
        "𝑒\n",
        "𝑛\n",
        "𝑡\n",
        ")\n",
        "−\n",
        "∑\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "𝑘\n",
        "𝑛\n",
        "𝑖\n",
        "𝑛\n",
        "×\n",
        "𝐸\n",
        "𝑛\n",
        "𝑡\n",
        "𝑟\n",
        "𝑜\n",
        "𝑝\n",
        "𝑦\n",
        "(\n",
        "𝐶\n",
        "ℎ\n",
        "𝑖\n",
        "𝑙\n",
        "𝑑\n",
        "𝑖\n",
        ")\n",
        "Information Gain=Entropy(Parent)−\n",
        "i=1\n",
        "∑\n",
        "k\n",
        "\t​\n",
        "\n",
        "n\n",
        "n\n",
        "i\n",
        "\t​\n",
        "\n",
        "\t​\n",
        "\n",
        "×Entropy(Child\n",
        "i\n",
        "\t​\n",
        "\n",
        ")\n",
        "\n",
        "where:\n",
        "\n",
        "𝐸\n",
        "𝑛\n",
        "𝑡\n",
        "𝑟\n",
        "𝑜\n",
        "𝑝\n",
        "𝑦\n",
        "(\n",
        "𝑃\n",
        "𝑎\n",
        "𝑟\n",
        "𝑒\n",
        "𝑛\n",
        "𝑡\n",
        ")\n",
        "Entropy(Parent): Entropy before the split\n",
        "\n",
        "𝐸\n",
        "𝑛\n",
        "𝑡\n",
        "𝑟\n",
        "𝑜\n",
        "𝑝\n",
        "𝑦\n",
        "(\n",
        "𝐶\n",
        "ℎ\n",
        "𝑖\n",
        "𝑙\n",
        "𝑑\n",
        "𝑖\n",
        ")\n",
        "Entropy(Child\n",
        "i\n",
        "\t​\n",
        "\n",
        "): Entropy of each child node after the split\n",
        "\n",
        "𝑛\n",
        "𝑖\n",
        "n\n",
        "i\n",
        "\t​\n",
        "\n",
        ": Number of samples in child node i\n",
        "\n",
        "𝑛\n",
        "n: Total number of samples in parent node\n",
        "\n",
        "\n",
        "**Importance of Information Gain**\n",
        "\n",
        "Feature Selection:\n",
        "Helps the tree choose the best feature at each node.\n",
        "\n",
        "Purity Improvement:\n",
        "Encourages splits that create homogeneous child nodes.\n",
        "\n",
        "Model Accuracy:\n",
        "Leads to better generalization and fewer classification errors.\n",
        "\n",
        "Core Idea of ID3 Algorithm:\n",
        "The ID3 (Iterative Dichotomiser 3) Decision Tree algorithm uses Information Gain as its main splitting criterion."
      ],
      "metadata": {
        "id": "GAvTkWZQtU_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 5: What are some common real-world applications of Decision Trees, and\n",
        "what are their main advantages and limitations?**\n",
        "\n",
        "- **Common Real-World Applications of Decision Trees**\n",
        "\n",
        "  -  **Customer Segmentation & Marketing**\n",
        "\n",
        "      - Used to identify which type of customers are most likely to buy a product.\n",
        "\n",
        "      - Example: Predict whether a customer will respond to a marketing campaign based on age, income, and past purchases.\n",
        "\n",
        "  - **Credit Risk Analysis (Finance)**\n",
        "\n",
        "      - Banks use decision trees to decide whether to approve or reject a loan.\n",
        "\n",
        "      - Example: Based on income, job stability, previous defaults, etc.\n",
        "\n",
        "  - **Medical Diagnosis**\n",
        "\n",
        "      - Used to help doctors classify diseases based on symptoms and medical test results.\n",
        "\n",
        "      - Example: Predicting whether a tumor is benign or malignant.\n",
        "\n",
        "  - **Human Resource (HR) Analytics**\n",
        "\n",
        "      - Predict whether an employee is likely to leave or stay in a company.\n",
        "\n",
        "      - Example: Based on salary, satisfaction score, and work hours.\n",
        "\n",
        "  - **Business Decision-Making**\n",
        "\n",
        "      - Companies use decision trees to simulate possible outcomes of business strategies.\n",
        "\n",
        "      - Example: Choosing between product launch strategies with different risks and profits.\n",
        "\n",
        "  - **E-commerce & Recommendation Systems**\n",
        "\n",
        "      - Used for predicting purchase behavior or recommending items.\n",
        "\n",
        "      - Example: Classifying users into segments based on their browsing and purchase history.\n",
        "\n",
        "---\n",
        "---\n",
        "| Advantage                             | Explanation                                                             |\n",
        "| ------------------------------------- | ----------------------------------------------------------------------- |\n",
        "| **Easy to interpret**                 | Can be visualized and explained like a flowchart.                       |\n",
        "| **No need for data normalization**    | Works well without scaling features.                                    |\n",
        "| **Handles both types of data**        | Works with **categorical and numerical** values.                        |\n",
        "| **Captures non-linear relationships** | Can split data in a non-linear way.                                     |\n",
        "| **Feature importance**                | Provides insight into which features are most important for prediction. |\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "---\n",
        "| Limitation                           | Explanation                                                                    |\n",
        "| ------------------------------------ | ------------------------------------------------------------------------------ |\n",
        "| **Overfitting**                      | Can become too complex and fit noise in the data (especially deep trees).      |\n",
        "| **Unstable**                         | Small changes in data can change the tree structure drastically.               |\n",
        "| **Biased towards dominant features** | Features with more unique values tend to be selected first.                    |\n",
        "| **Less accurate alone**              | Often less accurate compared to ensemble models like Random Forest or XGBoost. |\n"
      ],
      "metadata": {
        "id": "1qkcBuqttq_P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dataset Info:\n",
        "● Iris Dataset for classification tasks (sklearn.datasets.load_iris() or\n",
        "provided CSV).\n",
        "● Boston Housing Dataset for regression tasks\n",
        "(sklearn.datasets.load_boston() or provided CSV).\n",
        "Question 6: Write a Python program to:\n",
        "● Load the Iris Dataset\n",
        "● Train a Decision Tree Classifier using the Gini criterion\n",
        "● Print the model’s accuracy and feature importances\n",
        "(Include your Python code and output in the code box below.)**"
      ],
      "metadata": {
        "id": "bqJ4TQZnvpuv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred = clf.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"🌿 Decision Tree Classifier (Gini Criterion)\")\n",
        "print(\"--------------------------------------------\")\n",
        "print(\"Accuracy of the model: {:.2f}%\".format(accuracy * 100))\n",
        "print(\"\\nFeature Importances:\")\n",
        "for feature_name, importance in zip(iris.feature_names, clf.feature_importances_):\n",
        "    print(f\"{feature_name:25s} : {importance:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2qiRdUv_wB3R",
        "outputId": "201616ac-7486-4bd6-b52e-9636ce7deb87"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🌿 Decision Tree Classifier (Gini Criterion)\n",
            "--------------------------------------------\n",
            "Accuracy of the model: 100.00%\n",
            "\n",
            "Feature Importances:\n",
            "sepal length (cm)         : 0.0000\n",
            "sepal width (cm)          : 0.0167\n",
            "petal length (cm)         : 0.9061\n",
            "petal width (cm)          : 0.0772\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 7: Write a Python program to:\n",
        "● Load the Iris Dataset\n",
        "● Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to\n",
        "a fully-grown tree**"
      ],
      "metadata": {
        "id": "dwkDZn8WwN3S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Fully grown tree\n",
        "full_tree = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "full_tree.fit(X_train, y_train)\n",
        "y_pred_full = full_tree.predict(X_test)\n",
        "accuracy_full = accuracy_score(y_test, y_pred_full)\n",
        "\n",
        "# Tree with max_depth = 3\n",
        "limited_tree = DecisionTreeClassifier(criterion='gini', max_depth=3, random_state=42)\n",
        "limited_tree.fit(X_train, y_train)\n",
        "y_pred_limited = limited_tree.predict(X_test)\n",
        "accuracy_limited = accuracy_score(y_test, y_pred_limited)\n",
        "\n",
        "print(\"Decision Tree Accuracy Comparison\")\n",
        "print(\"------------------------------------\")\n",
        "print(f\"Fully-grown Tree Accuracy   : {accuracy_full * 100:.2f}%\")\n",
        "print(f\"Tree with max_depth=3       : {accuracy_limited * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tcr5oHAdwZ7Q",
        "outputId": "1de001ca-4841-4e15-82e2-c62b66c7fe88"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Accuracy Comparison\n",
            "------------------------------------\n",
            "Fully-grown Tree Accuracy   : 100.00%\n",
            "Tree with max_depth=3       : 100.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 8: Write a Python program to:\n",
        "● Load the Boston Housing Dataset\n",
        "● Train a Decision Tree Regressor\n",
        "● Print the Mean Squared Error (MSE) and feature importances**\n"
      ],
      "metadata": {
        "id": "ErhhvuMXwiQ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "data = fetch_california_housing()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "regressor = DecisionTreeRegressor(random_state=42)\n",
        "regressor.fit(X_train, y_train)\n",
        "\n",
        "y_pred = regressor.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "print(\"Decision Tree Regressor Results\")\n",
        "print(\"----------------------------------\")\n",
        "print(\"Mean Squared Error (MSE): {:.4f}\".format(mse))\n",
        "print(\"\\nFeature Importances:\")\n",
        "for feature_name, importance in zip(data.feature_names, regressor.feature_importances_):\n",
        "    print(f\"{feature_name:20s} : {importance:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kGad_QRpwqPJ",
        "outputId": "5eb24646-5aac-4792-8d56-0cee3827cbfd"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Regressor Results\n",
            "----------------------------------\n",
            "Mean Squared Error (MSE): 0.4952\n",
            "\n",
            "Feature Importances:\n",
            "MedInc               : 0.5285\n",
            "HouseAge             : 0.0519\n",
            "AveRooms             : 0.0530\n",
            "AveBedrms            : 0.0287\n",
            "Population           : 0.0305\n",
            "AveOccup             : 0.1308\n",
            "Latitude             : 0.0937\n",
            "Longitude            : 0.0829\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 9: Write a Python program to:\n",
        "● Load the Iris Dataset\n",
        "● Tune the Decision Tree’s max_depth and min_samples_split using\n",
        "GridSearchCV\n",
        "● Print the best parameters and the resulting model accuracy**"
      ],
      "metadata": {
        "id": "kmWUmT2UxX_e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "param_grid = {\n",
        "    'max_depth': [2, 3, 4, 5, None],\n",
        "    'min_samples_split': [2, 3, 4, 5, 10]\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=DecisionTreeClassifier(criterion='gini', random_state=42),\n",
        "    param_grid=param_grid,\n",
        "    cv=5,\n",
        "    scoring='accuracy'\n",
        ")\n",
        "\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Decision Tree Hyperparameter Tuning (GridSearchCV)\")\n",
        "print(\"----------------------------------------------------\")\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(\"Best Cross-Validation Accuracy: {:.2f}%\".format(grid_search.best_score_ * 100))\n",
        "print(\"Test Set Accuracy: {:.2f}%\".format(accuracy * 100))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "chcWE3xYxXst",
        "outputId": "145f770c-2538-4d3f-ceb5-375677445d62"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Hyperparameter Tuning (GridSearchCV)\n",
            "----------------------------------------------------\n",
            "Best Parameters: {'max_depth': 4, 'min_samples_split': 2}\n",
            "Best Cross-Validation Accuracy: 94.17%\n",
            "Test Set Accuracy: 100.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 10: Imagine you’re working as a data scientist for a healthcare company that\n",
        "wants to predict whether a patient has a certain disease. You have a large dataset with\n",
        "mixed data types and some missing values.\n",
        "Explain the step-by-step process you would follow to:\n",
        "● Handle the missing values\n",
        "● Encode the categorical features\n",
        "● Train a Decision Tree model\n",
        "● Tune its hyperparameters\n",
        "● Evaluate its performance\n",
        "And describe what business value this model could provide in the real-world\n",
        "setting**"
      ],
      "metadata": {
        "id": "tjDKkPZZxo2f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) Handle missing values\n",
        "\n",
        "Understand the missingness — check patterns and type:\n",
        "\n",
        "Missing Completely At Random (MCAR), Missing At Random (MAR), Missing Not At Random (MNAR).\n",
        "\n",
        "Use .isnull().mean() per column, cross-tab missingness vs target, and visualize with heatmaps/missingness matrices.\n",
        "\n",
        "Simple rules first\n",
        "\n",
        "If a feature is > 60–80% missing, consider dropping it (unless clinically important).\n",
        "\n",
        "If a row has very many missing values, consider dropping that row.\n",
        "\n",
        "Imputation strategies (use pipelines to avoid leakage):\n",
        "\n",
        "Numeric: median (robust), mean, or model-based (KNN, IterativeImputer/MICE) if relationships exist.\n",
        "\n",
        "Categorical: new category \"Missing\" or most-frequent.\n",
        "\n",
        "Indicator flags: add is_<feature>_missing boolean columns — they often help the model.\n",
        "\n",
        "Special handling for MNAR / clinically meaningful missingness: missing might itself be predictive (e.g., a test not ordered). Treat missingness as a feature.\n",
        "\n",
        "Always fit imputers on training data only (use Pipeline/ColumnTransformer so transforms are learned without leaking test info).\n",
        "\n",
        "2) Encode categorical features\n",
        "\n",
        "Assess cardinality:\n",
        "\n",
        "Low cardinality (≤ ~10 unique): One-Hot Encoding (use OneHotEncoder(handle_unknown='ignore')).\n",
        "\n",
        "High cardinality: target encoding / frequency encoding / hashing — but avoid target leakage; use methods that encode inside cross-validation folds or use TargetEncoder with CV.\n",
        "\n",
        "Rare categories: group rare levels into \"Other\" to avoid overfitting.\n",
        "\n",
        "Ordinal variables: use OrdinalEncoder only if order is meaningful.\n",
        "\n",
        "Use ColumnTransformer to apply different encoders to different columns inside a scikit-learn pipeline.\n",
        "\n",
        "3) Train a Decision Tree model\n",
        "\n",
        "Baseline model: fit a vanilla DecisionTreeClassifier(criterion='gini', random_state=42) on processed data to get a baseline.\n",
        "\n",
        "Address class imbalance (common in disease detection):\n",
        "\n",
        "Use class_weight='balanced' in the tree, or\n",
        "\n",
        "Resampling: SMOTE / ADASYN (for training folds only), or\n",
        "\n",
        "Use appropriate thresholds and cost matrices.\n",
        "\n",
        "Use cross-validation (stratified) to estimate performance (StratifiedKFold).\n",
        "\n",
        "4) Tune hyperparameters\n",
        "\n",
        "Important hyperparameters for DecisionTreeClassifier:\n",
        "\n",
        "max_depth, min_samples_split, min_samples_leaf, max_features, ccp_alpha (cost-complexity pruning), criterion.\n",
        "\n",
        "Search strategy:\n",
        "\n",
        "Use RandomizedSearchCV for large search spaces, GridSearchCV for smaller ones. Use cv=5 stratified folds.\n",
        "\n",
        "Optionally use nested CV to avoid optimistic bias when reporting final performance.\n",
        "\n",
        "Example grid:\n",
        "\n",
        "max_depth: [3, 5, 8, 12, None]\n",
        "\n",
        "min_samples_split: [2, 5, 10, 20]\n",
        "\n",
        "min_samples_leaf: [1, 2, 4, 8]\n",
        "\n",
        "max_features: [None, 'sqrt', 'log2']\n",
        "\n",
        "ccp_alpha: [0.0, 0.001, 0.01, 0.1]\n",
        "\n",
        "Metric to optimize: choose based on business need — e.g., recall (sensitivity) if missing a disease is costly, or a weighted metric like f1 if balance matters.\n",
        "\n",
        "5) Evaluate performance (beyond accuracy)\n",
        "\n",
        "Primary metrics (healthcare focus):\n",
        "\n",
        "Recall (Sensitivity): fraction of actual positives detected — often prioritized.\n",
        "\n",
        "Precision: proportion of predicted positives that are true — important to limit unnecessary followups.\n",
        "\n",
        "F1-score: harmonic mean of precision & recall.\n",
        "\n",
        "ROC-AUC for overall ranking performance.\n",
        "\n",
        "PR-AUC (preferred in imbalanced settings).\n",
        "\n",
        "Confusion matrix — always inspect to see tradeoffs (FP vs FN).\n",
        "\n",
        "Calibration — check predicted probabilities (reliability). Use calibration plots and CalibratedClassifierCV if needed.\n",
        "\n",
        "Decision thresholds: tune probability threshold to meet business constraints (e.g., achieve ≥90% sensitivity with acceptable precision).\n",
        "\n",
        "Clinical utility / cost analysis: translate FP/FN into expected costs (e.g., cost of missed disease vs cost of extra tests) and choose operating point that maximizes net benefit (decision curve analysis).\n",
        "\n",
        "Robustness checks:\n",
        "\n",
        "Evaluate on hold-out test set (never used in training/tuning).\n",
        "\n",
        "Test on subgroups (age, gender, location) to check fairness and performance drift.\n",
        "\n",
        "Perform bootstrap or repeated CV to estimate variance.\n",
        "\n",
        "Explainability: visualize the tree (sklearn.tree.plot_tree) and provide feature importances. For deeper insight use SHAP to explain individual predictions — crucial in healthcare.\n",
        "\n",
        "\n",
        "\n",
        "7) Example (compact sklearn pipeline)\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
        "\n",
        "num_cols = [...]        # list numeric cols\n",
        "cat_cols = [...]        # list categorical cols\n",
        "\n",
        "num_pipe = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler())          # optional for tree but OK\n",
        "])\n",
        "\n",
        "cat_pipe = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='constant', fill_value='Missing')),\n",
        "    ('ohe', OneHotEncoder(handle_unknown='ignore', sparse=False))\n",
        "])\n",
        "\n",
        "preproc = ColumnTransformer([\n",
        "    ('num', num_pipe, num_cols),\n",
        "    ('cat', cat_pipe, cat_cols)\n",
        "])\n",
        "\n",
        "pipe = Pipeline([\n",
        "    ('preproc', preproc),\n",
        "    ('clf', DecisionTreeClassifier(random_state=42, class_weight='balanced'))\n",
        "])\n",
        "\n",
        "param_grid = {\n",
        "    'clf__max_depth': [3,5,8,None],\n",
        "    'clf__min_samples_leaf': [1,2,4],\n",
        "    'clf__ccp_alpha': [0.0, 0.001, 0.01]\n",
        "}\n",
        "\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "grid = GridSearchCV(pipe, param_grid, cv=cv, scoring='recall', n_jobs=-1)\n",
        "grid.fit(X_train, y_train)\n",
        "\n"
      ],
      "metadata": {
        "id": "bumP1NB9x9b4"
      }
    }
  ]
}